{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e99366c0-c8ee-4732-b011-906c6d15769f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'ይነግሯቸዋል': {0: 1}})\n",
      "Document: 0, Cosine Similarity: 1.0\n",
      "defaultdict(<class 'list'>, {'ይነግሯቸዋል': {0: 1}})\n",
      "Document: 1, Cosine Similarity: 0.0\n",
      "defaultdict(<class 'list'>, {'ይነግሯቸዋል': {0: 1}})\n"
     ]
    }
   ],
   "source": [
    "from hm import anal\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from pdfminer.high_level import extract_text\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from math import sqrt\n",
    "amharic_punc_and_apos = r\"[፨፧፦!፥፤፣፣።፡,():;.\\'\\u1369-\\u137C]+\"\n",
    "amharic_punc_and_apos_pattern = re.compile(amharic_punc_and_apos)\n",
    "amharic_pattern = re.compile(r\"[\\u1200-\\u137F]+\")\n",
    "\n",
    "def remove_index_terms(tokens, percentage_to_remove=0.1):\n",
    "    word_counts = collections.Counter(tokens)\n",
    "    total_words = len(tokens)\n",
    "    sorted_counts = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    num_index_terms_to_remove = int(percentage_to_remove * len(sorted_counts))\n",
    "    \n",
    "    index_terms = [word for word, _ in sorted_counts[:num_index_terms_to_remove]]\n",
    "    \n",
    "    filtered_tokens = [token for token in tokens if token not in index_terms]\n",
    "    \n",
    "    return filtered_tokens\n",
    "def tokenize(text):\n",
    "    clean_text = amharic_punc_and_apos_pattern.sub(' ', text)\n",
    "    tokenized = amharic_pattern.findall(clean_text)\n",
    "    newtext=[]\n",
    "    for words in tokenized:\n",
    "        rep1=re.sub('[ሃኀኃሐሓኻ]','ሀ' ,words)\n",
    "        rep2=re.sub('[ሑኁዅ]','ሁ',rep1)\n",
    "        rep3=re.sub('[ኂሒኺ]','ሂ',rep2)\n",
    "        rep4=re.sub('[ኌሔዄ]','ሄ',rep3)\n",
    "        rep5=re.sub('[ሕኅ]','ህ',rep4)\n",
    "        rep6=re.sub('[ኆሖኾ]','ሆ',rep5)\n",
    "        rep7=re.sub('[ሠ]','ሰ',rep6)\n",
    "        rep8=re.sub('[ሡ]','ሱ',rep7)\n",
    "        rep9=re.sub('[ሢ]','ሲ',rep8)\n",
    "        rep10=re.sub('[ሣ]','ሳ',rep9)\n",
    "        rep11=re.sub('[ሤ]','ሴ',rep10)\n",
    "        rep12=re.sub('[ሥ]','ስ',rep11)\n",
    "        rep13=re.sub('[ሦ]','ሶ',rep12)\n",
    "        rep14=re.sub('[ዓኣዐ]','አ',rep13)\n",
    "        rep15=re.sub('[ዑ]','ኡ',rep14)\n",
    "        rep16=re.sub('[ዒ]','ኢ',rep15)\n",
    "        rep17=re.sub('[ዔ]','ኤ',rep16)\n",
    "        rep18=re.sub('[ዕ]','እ',rep17)\n",
    "        rep19=re.sub('[ዖ]','ኦ',rep18)\n",
    "        rep20=re.sub('[ጸ]','ፀ',rep19)\n",
    "        rep21=re.sub('[ጹ]','ፁ',rep20)\n",
    "        rep22=re.sub('[ጺ]','ፂ',rep21)\n",
    "        rep23=re.sub('[ጻ]','ፃ',rep22)\n",
    "        rep24=re.sub('[ጼ]','ፄ',rep23)\n",
    "        rep25=re.sub('[ጽ]','ፅ',rep24)\n",
    "        rep26=re.sub('[ጾ]','ፆ',rep25)     \n",
    "        rep27=re.sub('(ሉ[ዋአ])','ሏ',rep26)\n",
    "        rep28=re.sub('(ሙ[ዋአ])','ሟ',rep27)\n",
    "        rep29=re.sub('(ቱ[ዋአ])','ቷ',rep28)\n",
    "        rep30=re.sub('(ሩ[ዋአ])','ሯ',rep29)\n",
    "        rep31=re.sub('(ሱ[ዋአ])','ሷ',rep30)\n",
    "        rep32=re.sub('(ሹ[ዋአ])','ሿ',rep31)\n",
    "        rep33=re.sub('(ቁ[ዋአ])','ቋ',rep32)\n",
    "        rep34=re.sub('(ቡ[ዋአ])','ቧ',rep33)\n",
    "        rep35=re.sub('(ቹ[ዋአ])','ቿ',rep34)\n",
    "        rep36=re.sub('(ሁ[ዋአ])','ኋ',rep35)\n",
    "        rep37=re.sub('(ኑ[ዋአ])','ኗ',rep36)\n",
    "        rep38=re.sub('(ኙ[ዋአ])','ኟ',rep37)\n",
    "        rep39=re.sub('(ኩ[ዋአ])','ኳ',rep38)\n",
    "        rep40=re.sub('(ዙ[ዋአ])','ዟ',rep39)\n",
    "        rep41=re.sub('(ጉ[ዋአ])','ጓ',rep40)\n",
    "        rep42=re.sub('(ደ[ዋአ])','ዷ',rep41)\n",
    "        rep43=re.sub('(ጡ[ዋአ])','ጧ',rep42)\n",
    "        rep44=re.sub('(ጩ[ዋአ])','ጯ',rep43)\n",
    "        rep45=re.sub('(ጹ[ዋአ])','ጿ',rep44)\n",
    "        rep46=re.sub('(ፉ[ዋአ])','ፏ',rep45)\n",
    "        rep47=re.sub('[ቊ]','ቁ',rep46) #ቁ can be written as ቊ\n",
    "        rep48=re.sub('[ኵ]','ኩ',rep47)#ኩ can be also written as ኵ  \n",
    "        newtext=rep48\n",
    "    return newtext\n",
    "\n",
    "def Normalize_tokenize(text):\n",
    "    clean_text = amharic_punc_and_apos_pattern.sub(' ', text)\n",
    "    tokenized = amharic_pattern.findall(clean_text)\n",
    "    newtext=[]\n",
    "    for words in tokenized:\n",
    "        rep1=re.sub('[ሃኀኃሐሓኻ]','ሀ' ,words)\n",
    "        rep2=re.sub('[ሑኁዅ]','ሁ',rep1)\n",
    "        rep3=re.sub('[ኂሒኺ]','ሂ',rep2)\n",
    "        rep4=re.sub('[ኌሔዄ]','ሄ',rep3)\n",
    "        rep5=re.sub('[ሕኅ]','ህ',rep4)\n",
    "        rep6=re.sub('[ኆሖኾ]','ሆ',rep5)\n",
    "        rep7=re.sub('[ሠ]','ሰ',rep6)\n",
    "        rep8=re.sub('[ሡ]','ሱ',rep7)\n",
    "        rep9=re.sub('[ሢ]','ሲ',rep8)\n",
    "        rep10=re.sub('[ሣ]','ሳ',rep9)\n",
    "        rep11=re.sub('[ሤ]','ሴ',rep10)\n",
    "        rep12=re.sub('[ሥ]','ስ',rep11)\n",
    "        rep13=re.sub('[ሦ]','ሶ',rep12)\n",
    "        rep14=re.sub('[ዓኣዐ]','አ',rep13)\n",
    "        rep15=re.sub('[ዑ]','ኡ',rep14)\n",
    "        rep16=re.sub('[ዒ]','ኢ',rep15)\n",
    "        rep17=re.sub('[ዔ]','ኤ',rep16)\n",
    "        rep18=re.sub('[ዕ]','እ',rep17)\n",
    "        rep19=re.sub('[ዖ]','ኦ',rep18)\n",
    "        rep20=re.sub('[ጸ]','ፀ',rep19)\n",
    "        rep21=re.sub('[ጹ]','ፁ',rep20)\n",
    "        rep22=re.sub('[ጺ]','ፂ',rep21)\n",
    "        rep23=re.sub('[ጻ]','ፃ',rep22)\n",
    "        rep24=re.sub('[ጼ]','ፄ',rep23)\n",
    "        rep25=re.sub('[ጽ]','ፅ',rep24)\n",
    "        rep26=re.sub('[ጾ]','ፆ',rep25)     \n",
    "        rep27=re.sub('(ሉ[ዋአ])','ሏ',rep26)\n",
    "        rep28=re.sub('(ሙ[ዋአ])','ሟ',rep27)\n",
    "        rep29=re.sub('(ቱ[ዋአ])','ቷ',rep28)\n",
    "        rep30=re.sub('(ሩ[ዋአ])','ሯ',rep29)\n",
    "        rep31=re.sub('(ሱ[ዋአ])','ሷ',rep30)\n",
    "        rep32=re.sub('(ሹ[ዋአ])','ሿ',rep31)\n",
    "        rep33=re.sub('(ቁ[ዋአ])','ቋ',rep32)\n",
    "        rep34=re.sub('(ቡ[ዋአ])','ቧ',rep33)\n",
    "        rep35=re.sub('(ቹ[ዋአ])','ቿ',rep34)\n",
    "        rep36=re.sub('(ሁ[ዋአ])','ኋ',rep35)\n",
    "        rep37=re.sub('(ኑ[ዋአ])','ኗ',rep36)\n",
    "        rep38=re.sub('(ኙ[ዋአ])','ኟ',rep37)\n",
    "        rep39=re.sub('(ኩ[ዋአ])','ኳ',rep38)\n",
    "        rep40=re.sub('(ዙ[ዋአ])','ዟ',rep39)\n",
    "        rep41=re.sub('(ጉ[ዋአ])','ጓ',rep40)\n",
    "        rep42=re.sub('(ደ[ዋአ])','ዷ',rep41)\n",
    "        rep43=re.sub('(ጡ[ዋአ])','ጧ',rep42)\n",
    "        rep44=re.sub('(ጩ[ዋአ])','ጯ',rep43)\n",
    "        rep45=re.sub('(ጹ[ዋአ])','ጿ',rep44)\n",
    "        rep46=re.sub('(ፉ[ዋአ])','ፏ',rep45)\n",
    "        rep47=re.sub('[ቊ]','ቁ',rep46) #ቁ can be written as ቊ\n",
    "        rep48=re.sub('[ኵ]','ኩ',rep47)#ኩ can be also written as ኵ  \n",
    "        newtext.append(rep48)\n",
    "    return newtext\n",
    "def word_frequency(token,tokens):\n",
    "    word_freq = tokens.count(token)\n",
    "    #print(word_freq)\n",
    "    return word_freq \n",
    "with open(\"index.txt\", \"r\", encoding=\"utf-8\") as tobeinverted:\n",
    "    all_document_tokens=tobeinverted.read().split()\n",
    "\n",
    "def create_vector(token,tokens,document_id,all_document_tokens):\n",
    "    vector = defaultdict(list)\n",
    "    vector[token] = {document_id: word_frequency(token,tokens)}\n",
    "    return vector\n",
    "\n",
    "\n",
    "document_id_counter = 0\n",
    "listofdocumentVectors=[]\n",
    "for filename in os.listdir('C:/Users/Dell/Desktop/pyback'):\n",
    "  if filename.endswith(\".pdf\"):\n",
    "    inverted_index = defaultdict(list)\n",
    "    document_vector= defaultdict(list)\n",
    "    document_id = document_id_counter  # Assuming document_id_counter is defined elsewhere\n",
    "    document_id_counter += 1\n",
    "    text = extract_text(os.path.join('C:/Users/Dell/Desktop/pyback', filename))\n",
    "    tokens = Normalize_tokenize(text)\n",
    "    tokens_after_removal=remove_index_terms(tokens)\n",
    "    new=[]\n",
    "    for word in tokens:\n",
    "      stem=anal('amh',word)\n",
    "      if stem:\n",
    "          stemList1=stem[0]\n",
    "          stemList2=list(stemList1.values())\n",
    "          tokenized_word = tokenize(str(stemList2[1]))\n",
    "          if tokenized_word:  # Ensure tokenized_word is not empty\n",
    "            new.append(tokenized_word)\n",
    "    print(new)\n",
    "    for index in new:\n",
    "      if isinstance(index,list):\n",
    "          continue\n",
    "      inverted_index[index]={filename:word_frequency(index,new)}\n",
    "      document_vectors=create_vector(index,new,filename,all_document_tokens)\n",
    "    listofdocumentVectors.append(document_vectors)\n",
    "print(listofdocumentVectors)\n",
    "\n",
    "with open(\"data.pickle\",\"wb\") as file:\n",
    "    pickle.dump(listofdocumentVectors,file)\n",
    "with open(\"data.pickle\",\"rb\") as file:\n",
    "    data=pickle.load(file)\n",
    "\n",
    "    #print(document_vector)\n",
    "    #print(inverted_index)\n",
    "    #words used for indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a2284-a583-488c-beea-95b4921ed51e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
